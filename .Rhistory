uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ")")
}
writeLines(unlist(ncmd), "text.sql")
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");")
}
writeLines(unlist(ncmd), "text.sql")
ncmd
names(ncmd)
names(ak)
names(ncmd) <- names(ak)
tabindex %in% names(ak)
ctab[!tabindex %in% names(ak)]
tab2 <- c(ctab[!tabindex %in% names(ak)], ncmd)
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");")
}
names(ncmd) <- names(ak)
tab2 <- c(ctab[!tabindex %in% names(ak)], ncmd)
writeLines(unlist(tab2), "text.sql")
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");\n\n")
}
names(ncmd) <- names(ak)
tab2 <- c(ctab[!tabindex %in% names(ak)], ncmd)
writeLines(unlist(tab2), "text.sql")
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");\n\n")
}
names(ncmd) <- names(ak)
tab2 <- c(ctab[!tabindex %in% names(ak)], ncmd)
writeLines(unlist(tab2), "text.sql")
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");")
}
names(ncmd) <- names(ak)
tab2 <- c(ctab[!tabindex %in% names(ak)], ncmd)
writeLines(unlist(tab2), "text.sql")
tabnfk <- ctab[!tabindex %in% names(ak)]
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");\nn")
}
names(ncmd) <- names(ak)
tabnfk <- ctab[!tabindex %in% names(ak)]
lapply(tabnfk, c, ";\n")
tabnfk <- lapply(tabnfk, c, ";\n")
tab2 <- c(tabnfk, ncmd)
writeLines(unlist(tab2), "text.sql")
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");\n\n")
}
names(ncmd) <- names(ak)
tabnfk <- ctab[!tabindex %in% names(ak)]
tabnfk <- lapply(tabnfk, c, ";\n")
tab2 <- c(tabnfk, ncmd)
writeLines(unlist(tab2), "text.sql")
qr <- readLines("Downloads/QuickDBD-tumorLinePhosLandscape (1).sql")
qrs <- split(qr, cumsum(qr %in% c("GO", "")))
qrs <- lapply(qrs, function(x) x[!x %in% c("GO", "")])
ctab <- qrs[sapply(qrs, function(x) grepl("^CREATE TABLE", x[1]))]
afkey <- qrs[sapply(qrs, function(x) grepl("^ALTER TABLE", x[1]))]
tabindex <- sapply(ctab, function(x) strsplit(x[1], " ")[[1]][3])
ak <- t(sapply(afkey, function(x) {
if (grepl("ALTER TABLE", x[1])) {
name <- strsplit(x[1], " ")[[1]][3]
s <- regexpr("FOREIGN KEY", x[1])
q <- paste(substr(x[1], s, nchar(x[1])), x[2])
c(name, q)
}
}))
ak <- split(ak[, 2], ak[, 1])
ak <- sapply(ak, paste, collapse = ",\n")
length(ak)
ncmd <- list()
for (i in 1:length(ak)) {
uu <- names(ak)[i]
cmd <- ctab[[which(uu == tabindex)]]
n <- length(cmd)
cmd[n] <- sub(")", ",", cmd[n])
ncmd[[i]] <- c(cmd, ak[i], ");\n\n")
}
names(ncmd) <- names(ak)
tabnfk <- ctab[!tabindex %in% names(ak)]
tabnfk <- lapply(tabnfk, c, ";\n")
tab2 <- c(tabnfk, ncmd)
writeLines(unlist(tab2), "text.sql")
p1 <- c(4.1, 5.5, 1.2, 6.6, 9.0, 12.1)
grp <- c("cancer", "cancer", "cancer", "normal", "normal", "normal")
t.test(p1 ~ grp)
grp <- c(T, T, T, F, F, F)
t.test(p1 ~ grp)
19/29/4
install.packages("")
install.packages("~/Downloads/estimate_1.0.11.tar.gz", type="source", repos = NULL)
library("rvest")
library("RSelenium")
client <- rsDriver(port = 4576L, browser = "chrome")
client <- client$client
client$navigate("http://www.cancersecretome.org/search")
client <- rsDriver(port = 45443L, browser = "chrome")
client <- rsd$client
rsd<- client
client <- rsd$client
client$navigate("http://www.cancersecretome.org/search")
ll <- list()
for (i in 1:2710) {
if (i%%10 == 0)
print(i)
cElement <- client$findElement(using = 'id',"batch_next")
cElement$clickElement()
elem <- client$findElement(using = "id", "batch")
elemHtml <- elem$getElementAttribute("outerHTML")[[1]]
htmlObj <- read_html(elemHtml)
tt1 <- htmlObj %>% html_table()
ll[[i]] <- tt1[[1]]
}
ll2 <- list()
for (i in 1:687) {
if (i%%10 == 0)
print(i)
cElement <- client$findElement(using = 'id',"label_next")
cElement$clickElement()
# elem <- client$findElement(using = "id", "label")
elem <- client$findElement(using = "xpath", "//*[@id=\"label\"]")
elemHtml <- elem$getElementAttribute("outerHTML")[[1]]
# htmlObj <-
tt1 <- read_html(elemHtml) %>% html_table()
ll2[[i]] <- tt1[[1]]
}
ll2 <- list()
for (i in 1:687) {
if (i%%10 == 0)
print(i)
cElement <- client$findElement(using = 'id',"label_next")
cElement$clickElement()
# elem <- client$findElement(using = "id", "label")
elem <- client$findElement(using = "xpath", "//*[@id=\"label\"]")
elemHtml <- elem$getElementAttribute("outerHTML")[[1]]
# htmlObj <-
tt1 <- read_html(elemHtml) %>% html_table()
ll2[[i]] <- tt1[[1]]
}
ll2 <- list()
for (i in 1:687) {
if (i%%10 == 0)
print(i)
cElement <- client$findElement(using = 'id',"label_next")
cElement$clickElement()
# elem <- client$findElement(using = "id", "label")
elem <- client$findElement(using = "xpath", "//*[@id=\"label\"]")
elemHtml <- elem$getElementAttribute("outerHTML")[[1]]
# htmlObj <-
tt1 <- read_html(elemHtml) %>% html_table()
ll2[[i]] <- tt1[[1]]
}
tte <- t(sapply(ll2, dim))
tail(tte)
ll2[1]
elem <- client$findElement(using = "id", "batch")
elemHtml <- elem$getElementAttribute("outerHTML")[[1]]
htmlObj <- read_html(elemHtml)
tt1 <- htmlObj %>% html_table()
s1 <- tt1
# elem <- client$findElement(using = "id", "label")
elem <- client$findElement(using = "xpath", "//*[@id=\"label\"]")
elemHtml <- elem$getElementAttribute("outerHTML")[[1]]
# htmlObj <-
tt1 <- read_html(elemHtml) %>% html_table()
s2 <- tt1
s1
ll1 <- readRDS("~/Desktop/label.free.RDS")
ll1[[1]]
cc1 <- c(s1, ll1)
length(cc1)
tail(t(sapply(cc1, dim)))
cc2 <- c(s2, ll2)
ll1 <- cc1[-length(cc1)]
ll2 <- cc2[-length(cc2)]
tab1 <- do.call("rbind", ll1)
tab2 <- do.call("rbind", ll2)
head(tab1)
list(labelfree = tab1, labelbase = tab2)
tabc <- list(labelfree = tab1, labelbase = tab2)
saveRDS(tabc, file = "~/Desktop/HCSD.RDS")
colnames(tab1)
colnames(tab2)
colnames(tab1)
ts <- c("Nasopharyngeal carcinoma",
"Colorectal carcinoma",
"Liver cancer",
"Oral cancer",
"Bladder cancer",
"Breast cancer",
"Cervix cancer",
"Lung cancer",
"Pancreatic cancer",
"Epdermoid carcinoma",
"Lymphma",
"Ovarian Cancer",
"Glioblastoma",
"Prostate Cancer")
colnames(tab1)[colnames(tab1) == ""] <- ts
View(tab1)
colnames(tab1)[2:15]
tab1[2:15]
tab1$`Nasopharyngeal carcinoma`[1:6] == "↑"
tab1$`Liver cancer`[1:6] == "↑"
table(tab1$`Liver cancer`)
tab1[2:15] <- lapply(tab1[2:15], function(x) x == "↑")
head(tab1)
tabc <- list(labelfree = tab1, labelbase = tab2)
saveRDS(tabc, file = "~/Desktop/HCSD.RDS")
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
text <- readLines("~/Desktop/textminining/ak1.txt")
docs <- Corpus(VectorSource(text))
inspect(docs)
?inspect
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "-")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
head(d, 100)
head(d, 200)
grep("phos", d$word, value = TRUE)
grep("phos", d$word)
d[grep("phos", d$word), ]
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
text <- readLines("~/Desktop/textminining/ak1.txt")
docs <- Corpus(VectorSource(text))
inspect(docs)
text <- readLines("~/Desktop/textminining/ak1.txt")
text <- readLines("~/Desktop/textminining/ak1.txt")
text <- readLines("~/Desktop/textminining/ak1.txt")
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 100)
d[grep("phos", d$word), ]
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
head(d, 10)
proc <- function(x) {
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "-")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
}
text <- readLines("~/Desktop/textminining/ak1.txt")
proc(text)
proc <- function(x) {
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "-")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
}
text <- readLines("~/Desktop/textminining/ak1.txt")
proc(text)
a <- proc(text)
text <- readLines("~/Desktop/textminining/nelarabine.txt")
text
b <- proc(text)
head(a)
head(b)
head(a, n = 20)
head(b, n  = 20)
?stemDocument
stemDocument("bilingual")
stemDocument("bidirectional")
qq <- readLines("~/Downloads/uniprot_sprot.dat", n = 1000)
writeLines(qq, "~/Downloads/uniprot1000.txt")
?readLines
qq <- readLines("~/Downloads/uniprot_sprot.dat", n = 10000)
writeLines(qq, "~/Downloads/uniprot1000.txt")
library(tensorflow)
sess <- tf$Session()
hello <- tf$constant("hello, world!")
sess$run(hello)
a <- tf$constant(10)
b <- tf$constant(20)
sess$run(a + b)
sess$run(a + b)
sess$run(a + 1)
a
a$value_index
xdata <- runif(100, min = 0, max = 1)
ydata <- xdata * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L)), -1.0, 1.0)
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
shape(1)
?shape
shape(10, 2, 2)
y <- W * X_data + b
y <- W * xdata + b
loss <- tf$reduce_mean((y - ydata) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
library(tensorflow)
x_data <- runif(100, min=0, max=1)
y_data <- x_data * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
loss <- tf$reduce_mean((y - y_data) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
library(tensorflow)
xdata <- runif(100, min = 0, max = 1)
ydata <- xdata * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * xdata + b
loss <- tf$reduce_mean((y - ydata) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
for (step in 1:601) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
library(tensorflow)
xdata <- runif(100, min = 0, max = 1)
ydata <- xdata * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * xdata + b
loss <- tf$reduce_mean(abs(y - ydata))
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:601) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
loss <- tf$reduce_mean((y - ydata)^2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:601) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
setwd("~/mnt/msdata5/users_files/Chen/Projects/Concordance/R/omic3plus/")
library(devtools)
document("./R/")
as.integer(function(x) x+5)
readline()
?readline()
library(devtools)
document("./R/")
