docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
head(d, 100)
head(d, 200)
grep("phos", d$word, value = TRUE)
grep("phos", d$word)
d[grep("phos", d$word), ]
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
text <- readLines("~/Desktop/textminining/ak1.txt")
docs <- Corpus(VectorSource(text))
inspect(docs)
text <- readLines("~/Desktop/textminining/ak1.txt")
text <- readLines("~/Desktop/textminining/ak1.txt")
text <- readLines("~/Desktop/textminining/ak1.txt")
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 100)
d[grep("phos", d$word), ]
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
head(d, 10)
proc <- function(x) {
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "-")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
}
text <- readLines("~/Desktop/textminining/ak1.txt")
proc(text)
proc <- function(x) {
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "-")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
}
text <- readLines("~/Desktop/textminining/ak1.txt")
proc(text)
a <- proc(text)
text <- readLines("~/Desktop/textminining/nelarabine.txt")
text
b <- proc(text)
head(a)
head(b)
head(a, n = 20)
head(b, n  = 20)
?stemDocument
stemDocument("bilingual")
stemDocument("bidirectional")
qq <- readLines("~/Downloads/uniprot_sprot.dat", n = 1000)
writeLines(qq, "~/Downloads/uniprot1000.txt")
?readLines
qq <- readLines("~/Downloads/uniprot_sprot.dat", n = 10000)
writeLines(qq, "~/Downloads/uniprot1000.txt")
library(tensorflow)
sess <- tf$Session()
hello <- tf$constant("hello, world!")
sess$run(hello)
a <- tf$constant(10)
b <- tf$constant(20)
sess$run(a + b)
sess$run(a + b)
sess$run(a + 1)
a
a$value_index
xdata <- runif(100, min = 0, max = 1)
ydata <- xdata * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L)), -1.0, 1.0)
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
shape(1)
?shape
shape(10, 2, 2)
y <- W * X_data + b
y <- W * xdata + b
loss <- tf$reduce_mean((y - ydata) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
library(tensorflow)
x_data <- runif(100, min=0, max=1)
y_data <- x_data * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
loss <- tf$reduce_mean((y - y_data) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
library(tensorflow)
xdata <- runif(100, min = 0, max = 1)
ydata <- xdata * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * xdata + b
loss <- tf$reduce_mean((y - ydata) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
for (step in 1:601) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
library(tensorflow)
xdata <- runif(100, min = 0, max = 1)
ydata <- xdata * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * xdata + b
loss <- tf$reduce_mean(abs(y - ydata))
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:601) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
loss <- tf$reduce_mean((y - ydata)^2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess <- tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:601) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
library(datasets)
library(tensorflow)
data("iris")
sess <- tf$Session()
?tf$shape()
.deflatConcordance <- function(xcat, y, u, v, dmod = 1) {
if (dmod == 1) {
# deflation of S, crossprod matrix, the save with SVD directly
# the classical concordance approach
# S <- S - tcrossprod(u) %*% S
or the same
Ynorm <- Ynorm - Ynorm %*% tcrossprod(u)
} else if (dmod == 2) {
# deflaltion X using loading of X, as Lafosse & Hanafi 1997
# but not possible to incorporate with sparse factor
Xcat <- Xcat - Xcat %*% tcrossprod(v)
} else if (dmod == 3) {
# defaltion Y using its normed score
yb <- Ynorm %*% u
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
} else if (dmod == 4) {
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(yb)))
} else if (dmod == 5) {
xa <- Xcat %*% v
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(xa)))
} else {
stop("unknown deflation mode")
}
list(y = Ynorm, xcat = Xcat)
}
.deflatConcordance <- function(xcat, y, u, v, dmod = 1) {
if (dmod == 1) {
# deflation of S, crossprod matrix, the save with SVD directly
# the classical concordance approach
# S <- S - tcrossprod(u) %*% S
or the same
Ynorm <- Ynorm - Ynorm %*% tcrossprod(u)
} else if (dmod == 2) {
# deflaltion X using loading of X, as Lafosse & Hanafi 1997
# but not possible to incorporate with sparse factor
Xcat <- Xcat - Xcat %*% tcrossprod(v)
} else if (dmod == 3) {
# defaltion Y using its normed score
yb <- Ynorm %*% u
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
} else if (dmod == 4) {
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(yb)))
} else if (dmod == 5) {
xa <- Xcat %*% v
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(xa)))
} else {
stop("unknown deflation mode")
}
list(y = Ynorm, xcat = Xcat)
}
.deflatConcordance <- function(xcat, y, u, v, dmod = 1) {
if (dmod == 1) {
# deflation of S, crossprod matrix, the save with SVD directly
# the classical concordance approach
# S <- S - tcrossprod(u) %*% S
or the same
Ynorm <- Ynorm - Ynorm %*% tcrossprod(u)
} else if (dmod == 2) {
# deflaltion X using loading of X, as Lafosse & Hanafi 1997
# but not possible to incorporate with sparse factor
Xcat <- Xcat - Xcat %*% tcrossprod(v)
} else if (dmod == 3) {
# defaltion Y using its normed score
yb <- Ynorm %*% u
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
} else if (dmod == 4) {
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(yb)))
} else if (dmod == 5) {
xa <- Xcat %*% v
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(xa)))
} else {
stop("unknown deflation mode")
}
list(y = Ynorm, xcat = Xcat)
}
.deflatConcordance <- function(xcat, y, u, v, dmod = 1) {
if (dmod == 1) {
# deflation of S, crossprod matrix, the save with SVD directly
# the classical concordance approach
# S <- S - tcrossprod(u) %*% S
# or the same
Ynorm <- Ynorm - Ynorm %*% tcrossprod(u)
} else if (dmod == 2) {
# deflaltion X using loading of X, as Lafosse & Hanafi 1997
# but not possible to incorporate with sparse factor
Xcat <- Xcat - Xcat %*% tcrossprod(v)
} else if (dmod == 3) {
# defaltion Y using its normed score
yb <- Ynorm %*% u
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
} else if (dmod == 4) {
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(yb)))
} else if (dmod == 5) {
xa <- Xcat %*% v
yb <- Ynorm %*% u
# defaltion X and Y using normed score of Y, not suggested
Ynorm <- Ynorm - t(t(Ynorm) %*% tcrossprod(normvec(yb)))
Xcat <- Xcat - t(t(Xcat) %*% tcrossprod(normvec(xa)))
} else {
stop("unknown deflation mode")
}
list(y = Ynorm, xcat = Xcat)
}
library(parallel)
setwd("~/mnt/msdata5/users_files/Chen/Projects/Concordance/R/omic3plus/")
library(devtools)
document("./R/")
document("./R/")
document("./R/")
document("./R/")
cv.concord2 <- function(x, y, fold = 7, opt.kx = seq(0.1, 0.6, length.out = 10),
opt.ky = seq(0.1, 0.9, length.out = 10), ncores = 1, ...) {
r <- concord(x, y, ncomp = 1, verbose = FALSE, ...)
nobs <- ncol(y)
#
v <- sort(r$score.y[, 1])
m <- replicate(ceiling(nobs/fold), 1:fold)
m <- c(apply(m, 2, sample))[1:nobs]
#
cv <- mclapply(unique(m), mc.cores = ncores, function(i) {
cat(paste("Validating fold label", i, "...\n"))
ii <- m == i
xn <- lapply(r$normed$x, t)
yn <- t(r$normed$y)
xx <- lapply(xn, function(mat) mat[, ii])
yy <- yn[, ii]
xt <- lapply(xn, function(mat) mat[, !ii])
yt <- yn[, !ii]
xcat <- do.call(rbind, xt)
cvx <- sapply(opt.kx, function(kk) {
cat(paste("    kx =", kk, "...\n"))
r0 <- concord(xx, yy, ncomp = 1, dmod = 1, option = "uniform", center = FALSE, scale = FALSE, kx = kk, verbose = FALSE)
pred.x <- crossprod(xcat, object$loading.x)
pred.y <- crossprod(yt, object$loading.x)
cor(pred.x, pred.y)
# sum((yt - predictConcordance(r0, xt))^2)
})
cvy <- sapply(opt.ky, function(kk) {
cat(paste("    ky =", kk, "...\n"))
r0 <- concord(xx, yy, ncomp = 1, dmod = 1, option = "uniform", center = FALSE, scale = FALSE, ky = kk, verbose = FALSE)
pred.x <- crossprod(xcat, object$loading.x)
pred.y <- crossprod(yt, object$loading.x)
cor(pred.x, pred.y)
# sum((yt - predictConcordance(r0, xt))^2)
})
list(cvx = cvx, cvy = cvy)
})
cvx <- sapply(cv, "[[", "cvx")
cvy <- sapply(cv, "[[", "cvy")
mx <- rowMeans(cvx)
sdx <- rowSds(cvx)
my <- rowMeans(cvy)
sdy <- rowSds(cvy)
list(x = data.frame(kx = opt.kx, mean = mx, sd = sdx),
y = data.frame(ky = opt.ky, mean = my, sd = sdy),
cvx = cvx,
cvy = cvy)
}
?crossprod
document("./R/")
document("./R/")
document("./R/")
document("./R/")
?concord
cv.concord()
?cv.concord
#' @title Sparse concordance analysis with cross-validation
#'
#' @description Sparse concordance analysis, for each component, the optimal sparsity level is
#'   optimized using cross-validation
#'
#' @param x A list of predictive matrices
#' @param y the dependent matrix
#' @param opt.kx a numeric vector, candidate kx to be evaluated
#' @param opt.ky a numeric vector, candidate ky to be evaluated
#' @param fold the number of fold for cross validation
#' @param ncores the number of cores to be used, passed to \code{mclapply} in "parallel" package.
#' @param dmod see \code{\link{concord}}
#' @param center see \code{\link{concord}}
#' @param scale see \code{\link{concord}}
#' @param option see \code{\link{concord}}
#' @param kx see \code{\link{concord}}
#' @param ky see \code{\link{concord}}
#' @param wx see \code{\link{concord}}
#' @param wy see \code{\link{concord}}
#' @param pos see \code{\link{concord}}
#'
#' @author Chen Meng
#' @return concordance with results of optimization of kx, ky params
#' @importFrom parallel mclapply
#' @importFrom stats cor
#' @importFrom matrixStats rowSds rowMedians
#' @export
#' @examples
#'
sconcord <- function(x, y, opt.kx, opt.ky, fold = 5, ncores = 1,
dmod = 1, center = TRUE, scale = FALSE, option = "uniform",
wx = 1, wy = 1, pos = FALSE) {
call <- match.call()
"loading.x" <- "loading.y" <- "score.xsep" <-
"score.x" <- "score.y" <- "loading.x.index" <-
"score.x.index" <- "var" <- c()
# "normed" <- "deflated" <- list()
opt <- list()
for (i in 1:ncomp) {
cat(paste("calculating component", i, "...\n"))
cat("cross validation ...")
ctv <- cv.concord(x = x, y = y, fold = fold, opt.kx = opt.kx, opt.ky = opt.ky,
ncores = ncores, verbose = FALSE,
center = center, scale = scale, option = option,
dmod = 0, pos = pos, wx = wx, wy = wy)
okx <- opt.kx[which.max(rowMedians(ctv$cvx))]
oky <- opt.ky[which.max(rowMedians(ctv$cvy))]
cat("fit model ...")
res <- concord(x, y, ncomp = 1, kx = okx, ky = oky, verbose = FALSE,
center = center, scale = scale, option = option,
dmod = dmod, pos = pos, wx = wx, wy = wy)
enter <- FALSE
scale <- FALSE
option <- "uniform"
y <- res$deflated$y
x <- res$deflated$x
loading.x <- cbind(loading.x, res$loading.x)
loading.y <- cbind(loading.y, res$loading.y)
score.xsep <- cbind(score.xsep, res$score.xsep)
score.x <- cbind(score.x, res$score.x)
score.y <- cbind(score.y, res$score.y)
var <- c(var, res$var)
# normed[[i]] <- res$normed
# deflated[[i]] <- res$deflated
opt[[i]] <- ctv
}
loading.x.index <- res$loading.x.index
score.x.index <- res$score.x.index
list(loading.x = loading.x,
loading.y = loading.y,
score.xsep = score.xsep,
score.x = score.x,
score.y = score.y,
loading.x.index = loading.x.index,
score.x.index = score.x.index,
var = var,
cv = opt,
# normed = normed,
# deflated = deflated,
call = call)
}
document("./R/")
document("./R/")
document("./R/")
install_github("mengchen18/omic3plus")
?info
install_github("mengchen18/omic3plus")
